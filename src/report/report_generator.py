import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
from typing import Dict, Any, List
from pathlib import Path
import json
from src.utils.text_processing import clean_text_for_wordcloud
from src.utils.plotting import setup_korean_font, get_korean_font_prop

class ReportGenerator:
    """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.output_config = config['output']
        
    def generate_wordcloud(self, texts: List[str], title: str, output_path: str):
        """ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ê²°í•©
        cleaned_texts = []
        for text in texts:
            cleaned_text = clean_text_for_wordcloud(text)
            if cleaned_text:
                cleaned_texts.append(cleaned_text)
        
        if not cleaned_texts:
            print(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. {output_path}")
            return
        
        combined_text = ' '.join(cleaned_texts)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            wordcloud = WordCloud(
                font_path='/System/Library/Fonts/AppleGothic.ttf',  # macOS
                width=800,
                height=400,
                background_color='white',
                max_words=50,  # ë” ì ì€ ë‹¨ì–´ë¡œ ê¹”ë”í•˜ê²Œ
                colormap='viridis',
                min_font_size=10,
                max_font_size=100,
                relative_scaling=0.5,
                random_state=42
            ).generate(combined_text)
        except:
            # í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì„¤ì •
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=50,
                colormap='viridis',
                min_font_size=10,
                max_font_size=100,
                relative_scaling=0.5,
                random_state=42
            ).generate(combined_text)
        
        plt.figure(figsize=(10, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(title, fontsize=16, pad=20)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    def generate_frequency_chart(self, data: pd.DataFrame, x_col: str, y_col: str, 
                               title: str, output_path: str):
        """ë¹ˆë„ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        setup_korean_font()
        font_prop = get_korean_font_prop()
        
        plt.figure(figsize=(12, 6))
        sns.barplot(data=data, x=x_col, y=y_col)
        plt.title(title, fontproperties=font_prop, fontsize=16)
        plt.xlabel('í´ëŸ¬ìŠ¤í„°', fontproperties=font_prop, fontsize=12)
        plt.ylabel('ë¬¸ì„œ ìˆ˜', fontproperties=font_prop, fontsize=12)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ë¹ˆë„ ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    def generate_markdown_report(self, topic_results: Dict[str, Any], 
                               cluster_results: Dict[str, Any],
                               analysis_results: List[Dict[str, Any]],
                               output_dir: str):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        report_content = self._create_report_content(
            topic_results, cluster_results, analysis_results
        )
        
        report_path = Path(output_dir) / "ë¶„ì„_ë¦¬í¬íŠ¸.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")
    
    def _create_report_content(self, topic_results: Dict[str, Any],
                             cluster_results: Dict[str, Any],
                             analysis_results: List[Dict[str, Any]]) -> str:
        """ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        content = f"""# ìƒë‹´/ê²Œì‹œíŒ ë°ì´í„° ì£¼ì œ ë°œê²¬ ë° ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š ë¶„ì„ ê°œìš”

ì´ ë¦¬í¬íŠ¸ëŠ” ìƒë‹´/ê²Œì‹œíŒ ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ìë™í™”ëœ ì£¼ì œ ë°œê²¬ ë° ì‹¬ì¸µ ë¶„ì„ì„ ìˆ˜í–‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

### ë¶„ì„ ë°©ë²•ë¡ 
- **ì£¼ì œ ë°œê²¬**: LDA (Latent Dirichlet Allocation) ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
- **í´ëŸ¬ìŠ¤í„°ë§**: K-means í´ëŸ¬ìŠ¤í„°ë§ + Sentence-BERT ì„ë² ë”©
- **ì‹¬ì¸µ ë¶„ì„**: LLM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„

## ğŸ¯ ì£¼ì œ ë°œê²¬ ê²°ê³¼

### ì „ì²´ ì£¼ì œ ë¶„í¬
ì´ {len(topic_results.get('top_terms', {}))}ê°œì˜ ì£¼ìš” ì£¼ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.

"""
        
        # ì£¼ì œë³„ ìƒì„¸ ì •ë³´
        if 'top_terms' in topic_results:
            content += "### ì£¼ì œë³„ ìƒìœ„ í‚¤ì›Œë“œ\n\n"
            for topic_id, terms in topic_results['top_terms'].items():
                content += f"#### ì£¼ì œ {topic_id + 1}\n"
                content += "**ìƒìœ„ í‚¤ì›Œë“œ:** "
                content += ", ".join([term for term, _ in terms[:5]])
                content += "\n\n"
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        if cluster_results:
            content += "## ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼\n\n"
            content += f"ì´ {len(cluster_results.get('cluster_stats', []))}ê°œì˜ í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            
            for _, cluster in cluster_results.get('cluster_stats', []).iterrows():
                content += f"### {cluster['cluster_name']}\n"
                content += f"- **ë¬¸ì„œ ìˆ˜**: {cluster['document_count']}ê°œ\n"
                content += f"- **í‰ê·  ìœ ì‚¬ë„**: {cluster['avg_similarity']:.3f}\n"
                content += f"- **ëŒ€í‘œ ë¬¸ì„œ**: {cluster['representative_text'][:100]}...\n\n"
        
        # LLM ë¶„ì„ ê²°ê³¼
        if analysis_results:
            content += "## ğŸ¤– LLM ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼\n\n"
            
            for result in analysis_results:
                cluster_name = result['cluster_name']
                topic_name = result['topic_name']
                summary = result['cluster_summary']
                
                content += f"### {cluster_name} ({topic_name})\n\n"
                content += "#### ì£¼ìš” ë¶„ì„ ê²°ê³¼\n"
                content += f"- **ì£¼ìš” ì›ì¸**: {summary.get('ì£¼ìš”_ì›ì¸', 'ë¶„ì„ ë¶ˆê°€')}\n"
                content += f"- **ì£¼ìš” í–‰ìœ„ì**: {summary.get('ì£¼ìš”_í–‰ìœ„ì', 'ë¶„ì„ ë¶ˆê°€')}\n"
                content += f"- **ê³µí†µ ìš”êµ¬ì‚¬í•­**: {summary.get('ê³µí†µ_ìš”êµ¬ì‚¬í•­', 'ë¶„ì„ ë¶ˆê°€')}\n"
                content += f"- **ì „ì²´ í†¤**: {summary.get('ì „ì²´_í†¤', 'ë¶„ì„ ë¶ˆê°€')}\n"
                content += f"- **ì£¼ìš” ë¦¬ìŠ¤í¬**: {summary.get('ì£¼ìš”_ë¦¬ìŠ¤í¬', 'ë¶„ì„ ë¶ˆê°€')}\n"
                content += f"- **í•´ê²° ìš°ì„ ìˆœìœ„**: {summary.get('í•´ê²°_ìš°ì„ ìˆœìœ„', 'ë¶„ì„ ë¶ˆê°€')}\n"
                content += f"- **ì •ì±… ê°œì„ ì **: {summary.get('ì •ì±…_ê°œì„ ì ', 'ë¶„ì„ ë¶ˆê°€')}\n\n"
        
        # ì •ì±… ì œì–¸
        content += """## ğŸ’¡ ì •ì±… ì œì–¸

### ì£¼ìš” ë°œê²¬ì‚¬í•­
1. **ì£¼ì œë³„ ë¶„í¬**: ìƒë‹´ ë°ì´í„°ì—ì„œ íŠ¹ì • ì£¼ì œë“¤ì´ ì§‘ì¤‘ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¨
2. **í´ëŸ¬ìŠ¤í„° íŠ¹ì„±**: ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê³ ìœ í•œ ë¬¸ì œ íŒ¨í„´ ë°œê²¬
3. **LLM ë¶„ì„**: ì •ëŸ‰ì  ë¶„ì„ì„ í†µí•œ ì •ì±… ê°œì„  ë°©í–¥ ì œì‹œ

### ì •ì±… ê°œì„  ë°©ì•ˆ
- **ì¦‰ì‹œ ëŒ€ì‘**: ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ë¬¸ì œë“¤ì— ëŒ€í•œ ì¦‰ì‹œ ëŒ€ì‘ ì²´ê³„ êµ¬ì¶•
- **ì˜ˆë°©ì  ì¡°ì¹˜**: ë°˜ë³µ ë°œìƒí•˜ëŠ” ë¬¸ì œì— ëŒ€í•œ ì˜ˆë°©ì  ì •ì±… ìˆ˜ë¦½
- **ëª¨ë‹ˆí„°ë§ ê°•í™”**: ì£¼ì œë³„ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•

## ğŸ“ˆ ì‹œê°í™” ê²°ê³¼

ë¶„ì„ ê³¼ì •ì—ì„œ ìƒì„±ëœ ì‹œê°í™” ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ì£¼ì œ ë¶„í¬ ì°¨íŠ¸
- í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ìˆ˜ ë¶„í¬
- ì›Œë“œí´ë¼ìš°ë“œ
- ë¹ˆë„ ë¶„ì„ ì°¨íŠ¸

## ğŸ”§ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### ì‚¬ìš©ëœ ê¸°ìˆ 
- **ì£¼ì œ ë°œê²¬**: scikit-learn LDA
- **ì„ë² ë”©**: Sentence-BERT
- **í´ëŸ¬ìŠ¤í„°ë§**: K-means
- **í…ìŠ¤íŠ¸ ë¶„ì„**: OpenAI GPT ëª¨ë¸
- **ì‹œê°í™”**: matplotlib, seaborn, wordcloud

### ë°ì´í„° ì²˜ë¦¬
- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ê·œí™”
- ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
- ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ ê´€ë¦¬

---
*ì´ ë¦¬í¬íŠ¸ëŠ” ìë™í™”ëœ ë¶„ì„ ì‹œìŠ¤í…œì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        
        return content
    
    def generate_all_visualizations(self, topic_results: Dict[str, Any],
                                   cluster_results: Dict[str, Any],
                                   texts: List[str],
                                   output_dir: str):
        """ëª¨ë“  ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        output_path = Path(output_dir)
        
        # ì£¼ì œë³„ ì›Œë“œí´ë¼ìš°ë“œ
        if 'top_terms' in topic_results:
            # ì£¼ì œ í• ë‹¹ ê²°ê³¼ ë¡œë“œ
            topic_assignments_path = Path(output_dir) / "csv" / "topic_assignments.csv"
            if topic_assignments_path.exists():
                topic_assignments = pd.read_csv(topic_assignments_path)
                
                for topic_id, terms in topic_results['top_terms'].items():
                    # í•´ë‹¹ ì£¼ì œì— í• ë‹¹ëœ ë¬¸ì„œë“¤ë§Œ í•„í„°ë§
                    topic_mask = topic_assignments['topic_id'] == topic_id
                    topic_doc_ids = topic_assignments[topic_mask]['document_id'].tolist()
                    topic_texts = [texts[doc_id] for doc_id in topic_doc_ids if doc_id < len(texts)]
                    
                    if topic_texts:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
                        wordcloud_path = output_path / "visualizations" / "wordclouds" / f"topic_{topic_id+1}_wordcloud.png"
                        self.generate_wordcloud(
                            topic_texts,
                            f"Topic {topic_id+1} Word Cloud",
                            str(wordcloud_path)
                        )
                    else:
                        print(f"ì£¼ì œ {topic_id+1}ì— í• ë‹¹ëœ ë¬¸ì„œê°€ ì—†ì–´ì„œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                print("ì£¼ì œ í• ë‹¹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                for topic_id, terms in topic_results['top_terms'].items():
                    wordcloud_path = output_path / "visualizations" / "wordclouds" / f"topic_{topic_id+1}_wordcloud.png"
                    self.generate_wordcloud(
                        texts,
                        f"Topic {topic_id+1} Word Cloud",
                        str(wordcloud_path)
                    )
                
                wordcloud_path = output_path / "visualizations" / "wordclouds" / f"topic_{topic_id+1}_wordcloud.png"
                self.generate_wordcloud(
                    topic_texts,
                    f"Topic {topic_id+1} Word Cloud",
                    str(wordcloud_path)
                )
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ì›Œë“œí´ë¼ìš°ë“œ
        if 'cluster_stats' in cluster_results:
            for _, cluster in cluster_results['cluster_stats'].iterrows():
                cluster_texts = cluster['sample_texts']
                
                wordcloud_path = output_path / "visualizations" / "wordclouds" / f"{cluster['cluster_name']}_wordcloud.png"
                self.generate_wordcloud(
                    cluster_texts,
                    f"{cluster['cluster_name']} Word Cloud",
                    str(wordcloud_path)
                )
        
        # ë¹ˆë„ ì°¨íŠ¸
        if 'cluster_stats' in cluster_results:
            freq_path = output_path / "visualizations" / "frequency_charts" / "cluster_frequency.png"
            self.generate_frequency_chart(
                cluster_results['cluster_stats'],
                'cluster_name',
                'document_count',
                'Cluster Distribution by Document Count',
                str(freq_path)
            )
        
        print("ëª¨ë“  ì‹œê°í™” ìƒì„± ì™„ë£Œ") 