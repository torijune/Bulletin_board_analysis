import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import json
import time
from typing import Dict, Any, List, Tuple
import openai
from openai import OpenAI
import os
from dotenv import load_dotenv
from src.utils.plotting import setup_korean_font, get_korean_font_prop

class TopicAnalysis:
    """ì£¼ì œë³„ ìƒì„¸ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm_client = None
        
        # .env íŒŒì¼ ë¡œë“œ
        load_dotenv()
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.llm_client = OpenAI(api_key=api_key)
                print("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def load_topic_data(self, csv_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ì£¼ì œ ê´€ë ¨ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("ì£¼ì œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì£¼ì œë³„ ë¬¸ì„œ í• ë‹¹ ë°ì´í„°
        topic_assignments = pd.read_csv(f"{csv_dir}/topic_assignments.csv")
        
        # ì£¼ì œë³„ í‚¤ì›Œë“œ ë°ì´í„°
        topic_keywords = pd.read_csv(f"{csv_dir}/global_topics_top_terms.csv")
        
        # ì „ì²˜ë¦¬ëœ ì›ë³¸ ë°ì´í„°
        preprocessed_data = pd.read_csv(f"{csv_dir}/preprocessed_data.csv")
        
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
        print(f"  - ì£¼ì œ í• ë‹¹: {len(topic_assignments)}ê°œ ë¬¸ì„œ")
        print(f"  - ì£¼ì œ í‚¤ì›Œë“œ: {len(topic_keywords)}ê°œ í‚¤ì›Œë“œ")
        print(f"  - ì›ë³¸ ë°ì´í„°: {len(preprocessed_data)}ê°œ ë¬¸ì„œ")
        
        return topic_assignments, topic_keywords, preprocessed_data
    
    def extract_topic_texts(self, topic_assignments: pd.DataFrame, 
                          preprocessed_data: pd.DataFrame, 
                          topic_id: int) -> List[str]:
        """íŠ¹ì • ì£¼ì œì— í• ë‹¹ëœ ë¬¸ì„œë“¤ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # í•´ë‹¹ ì£¼ì œì— í• ë‹¹ëœ ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
        topic_docs = topic_assignments[topic_assignments['topic_id'] == topic_id]
        doc_indices = topic_docs['document_id'].tolist()
        
        # ì›ë³¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë¬¸ì„œë“¤ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        topic_texts = []
        for idx in doc_indices:
            if idx < len(preprocessed_data):
                text = preprocessed_data.iloc[idx]['cleaned_text']
                topic_texts.append(text)
        
        return topic_texts
    
    def create_topic_wordcloud(self, texts: List[str], topic_name: str, 
                             output_dir: str, topic_id: int) -> str:
        """ì£¼ì œë³„ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"ì£¼ì œ {topic_id} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = ' '.join(texts)
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        words = combined_text.split()
        word_freq = Counter(words)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        setup_korean_font()
        font_prop = get_korean_font_prop()
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(
            font_path=font_prop.get_file(),
            width=800,
            height=600,
            background_color='white',
            max_words=100,
            colormap='viridis',
            prefer_horizontal=0.7
        ).generate_from_frequencies(word_freq)
        
        # ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥
        plt.figure(figsize=(12, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'ì£¼ì œ {topic_id}: {topic_name} ì›Œë“œí´ë¼ìš°ë“œ', 
                 fontproperties=font_prop, fontsize=16, pad=20)
        
        filename = f"{output_dir}/wordclouds/topic_{topic_id}_wordcloud.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename
    
    def analyze_topic_with_llm(self, texts: List[str], topic_name: str, 
                             topic_keywords: List[str]) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì œë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."""
        if self.llm_client is None:
            print("LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ì„œ ê¸°ë³¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self._basic_topic_analysis(texts, topic_name, topic_keywords)
        
        print(f"ì£¼ì œ '{topic_name}' LLM ë¶„ì„ ì¤‘...")
        
        # ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
        analysis_texts = texts[:10]  # ìƒìœ„ 10ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
        sample_texts = '\n'.join([f"- {text[:200]}..." for text in analysis_texts[:5]])
        
        # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
ë‹¤ìŒ ìƒë‹´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ì£¼ì œëª…: {topic_name}
ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(topic_keywords[:10])}

ì£¼ì œì— í¬í•¨ëœ ìƒë‹´ ë‚´ìš© ì˜ˆì‹œ:
{sample_texts}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON í˜•íƒœë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{{
    "ì£¼ì œ_ìš”ì•½": "ì´ ì£¼ì œì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
    "ì£¼ìš”_ë¬¸ì œì ": ["ë¬¸ì œì 1", "ë¬¸ì œì 2", "ë¬¸ì œì 3"],
    "ê´€ë ¨_í–‰ìœ„ì": ["í–‰ìœ„ì1", "í–‰ìœ„ì2", "í–‰ìœ„ì3"],
    "í•´ê²°_ë°©ì•ˆ": ["ë°©ì•ˆ1", "ë°©ì•ˆ2", "ë°©ì•ˆ3"],
    "ì •ì±…_ì‹œì‚¬ì ": "ì •ì±…ì  ê´€ì ì—ì„œì˜ ì‹œì‚¬ì ",
    "ìš°ì„ ìˆœìœ„": "ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ",
    "ì˜ˆìƒ_ë¹ˆë„": "ìƒë‹´ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ ì˜ˆìƒ"
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = self.llm_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ìƒë‹´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            # JSON íŒŒì‹±
            analysis_text = response.choices[0].message.content.strip()
            analysis_result = json.loads(analysis_text)
            
            # ì¶”ê°€ ì •ë³´ ì¶”ê°€
            analysis_result['ë¶„ì„_ë¬¸ì„œ_ìˆ˜'] = len(texts)
            analysis_result['ì£¼ì œëª…'] = topic_name
            analysis_result['ì£¼ìš”_í‚¤ì›Œë“œ'] = topic_keywords[:10]
            
            print(f"LLM ë¶„ì„ ì™„ë£Œ: {topic_name}")
            return analysis_result
            
        except Exception as e:
            print(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._basic_topic_analysis(texts, topic_name, topic_keywords)
    
    def _basic_topic_analysis(self, texts: List[str], topic_name: str, 
                            topic_keywords: List[str]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì£¼ì œ ë¶„ì„ (LLM ì—†ì´)"""
        print(f"ì£¼ì œ '{topic_name}' ê¸°ë³¸ ë¶„ì„ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ í†µê³„
        text_lengths = [len(text) for text in texts]
        avg_length = np.mean(text_lengths)
        
        # ë‹¨ì–´ ë¹ˆë„ ë¶„ì„
        combined_text = ' '.join(texts)
        words = combined_text.split()
        word_freq = Counter(words)
        top_words = word_freq.most_common(10)
        
        return {
            'ì£¼ì œëª…': topic_name,
            'ë¶„ì„_ë¬¸ì„œ_ìˆ˜': len(texts),
            'ì£¼ìš”_í‚¤ì›Œë“œ': topic_keywords[:10],
            'í‰ê· _í…ìŠ¤íŠ¸_ê¸¸ì´': round(avg_length, 1),
            'ìƒìœ„_ë‹¨ì–´': [word for word, freq in top_words],
            'ì£¼ì œ_ìš”ì•½': f"{topic_name} ê´€ë ¨ ìƒë‹´ì´ {len(texts)}ê±´ ìˆìœ¼ë©°, í‰ê·  {round(avg_length, 1)}ì ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë¨",
            'ì£¼ìš”_ë¬¸ì œì ': ["ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œëŠ” êµ¬ì²´ì  ë¬¸ì œì  íŒŒì•… ì–´ë ¤ì›€"],
            'ê´€ë ¨_í–‰ìœ„ì': ["ìƒë‹´ì¸", "ê´€ë¦¬ì†Œì¥", "ì ìœ ì"],
            'í•´ê²°_ë°©ì•ˆ': ["LLM ë¶„ì„ í•„ìš”"],
            'ì •ì±…_ì‹œì‚¬ì ': "ìƒì„¸í•œ LLM ë¶„ì„ì„ í†µí•´ ì •ì±…ì  ì‹œì‚¬ì  ë„ì¶œ í•„ìš”",
            'ìš°ì„ ìˆœìœ„': "ì¤‘ê°„",
            'ì˜ˆìƒ_ë¹ˆë„': f"ì „ì²´ì˜ ì•½ {len(texts)}ê±´"
        }
    
    def create_topic_statistics(self, topic_assignments: pd.DataFrame, 
                              topic_keywords: pd.DataFrame) -> pd.DataFrame:
        """ì£¼ì œë³„ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("ì£¼ì œë³„ í†µê³„ ìƒì„± ì¤‘...")
        
        topic_stats = []
        
        for topic_id in topic_assignments['topic_id'].unique():
            # í•´ë‹¹ ì£¼ì œì˜ ë¬¸ì„œ ìˆ˜
            topic_docs = topic_assignments[topic_assignments['topic_id'] == topic_id]
            doc_count = len(topic_docs)
            
            # í•´ë‹¹ ì£¼ì œì˜ í‚¤ì›Œë“œ
            topic_kw = topic_keywords[topic_keywords['topic_id'] == topic_id]
            top_keywords = topic_kw.nlargest(5, 'score')['term'].tolist()
            
            # í‰ê·  ì‹ ë¢°ë„
            avg_confidence = topic_docs['confidence'].mean()
            
            topic_stats.append({
                'topic_id': topic_id,
                'topic_name': topic_docs['topic_name'].iloc[0],
                'document_count': doc_count,
                'avg_confidence': round(avg_confidence, 3),
                'top_keywords': ', '.join(top_keywords),
                'percentage': round(doc_count / len(topic_assignments) * 100, 1)
            })
        
        stats_df = pd.DataFrame(topic_stats)
        return stats_df
    
    def generate_topic_report(self, topic_analyses: Dict[int, Dict[str, Any]], 
                            topic_stats: pd.DataFrame, output_dir: str):
        """ì£¼ì œë³„ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("ì£¼ì œë³„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_content = []
        report_content.append("# ì£¼ì œë³„ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸\n")
        report_content.append(f"ìƒì„±ì¼ì‹œ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ì „ì²´ ìš”ì•½
        total_docs = topic_stats['document_count'].sum()
        report_content.append("## ğŸ“Š ì „ì²´ ìš”ì•½\n")
        report_content.append(f"- **ì´ ë¬¸ì„œ ìˆ˜**: {total_docs:,}ê°œ\n")
        report_content.append(f"- **ë°œê²¬ëœ ì£¼ì œ ìˆ˜**: {len(topic_stats)}ê°œ\n")
        report_content.append(f"- **í‰ê·  ì£¼ì œë‹¹ ë¬¸ì„œ ìˆ˜**: {total_docs/len(topic_stats):.1f}ê°œ\n\n")
        
        # ì£¼ì œë³„ ìƒì„¸ ë¶„ì„
        for _, topic_stat in topic_stats.iterrows():
            topic_id = topic_stat['topic_id']
            topic_name = topic_stat['topic_name']
            
            if topic_id in topic_analyses:
                analysis = topic_analyses[topic_id]
                
                report_content.append(f"## ğŸ¯ ì£¼ì œ {topic_id}: {topic_name}\n")
                report_content.append(f"**ë¬¸ì„œ ìˆ˜**: {topic_stat['document_count']}ê°œ ({topic_stat['percentage']}%)\n")
                report_content.append(f"**í‰ê·  ì‹ ë¢°ë„**: {topic_stat['avg_confidence']}\n\n")
                
                # LLM ë¶„ì„ ê²°ê³¼
                report_content.append("### ğŸ“‹ ìƒì„¸ ë¶„ì„\n")
                report_content.append(f"**ì£¼ì œ ìš”ì•½**: {analysis.get('ì£¼ì œ_ìš”ì•½', 'N/A')}\n\n")
                
                # ì£¼ìš” ë¬¸ì œì 
                problems = analysis.get('ì£¼ìš”_ë¬¸ì œì ', [])
                if problems:
                    report_content.append("**ì£¼ìš” ë¬¸ì œì **:\n")
                    for problem in problems:
                        report_content.append(f"- {problem}\n")
                    report_content.append("\n")
                
                # ê´€ë ¨ í–‰ìœ„ì
                actors = analysis.get('ê´€ë ¨_í–‰ìœ„ì', [])
                if actors:
                    report_content.append("**ê´€ë ¨ í–‰ìœ„ì**:\n")
                    for actor in actors:
                        report_content.append(f"- {actor}\n")
                    report_content.append("\n")
                
                # í•´ê²° ë°©ì•ˆ
                solutions = analysis.get('í•´ê²°_ë°©ì•ˆ', [])
                if solutions:
                    report_content.append("**í•´ê²° ë°©ì•ˆ**:\n")
                    for solution in solutions:
                        report_content.append(f"- {solution}\n")
                    report_content.append("\n")
                
                # ì •ì±… ì‹œì‚¬ì 
                policy = analysis.get('ì •ì±…_ì‹œì‚¬ì ', 'N/A')
                report_content.append(f"**ì •ì±… ì‹œì‚¬ì **: {policy}\n\n")
                
                # ìš°ì„ ìˆœìœ„ ë° ë¹ˆë„
                priority = analysis.get('ìš°ì„ ìˆœìœ„', 'N/A')
                frequency = analysis.get('ì˜ˆìƒ_ë¹ˆë„', 'N/A')
                report_content.append(f"**ìš°ì„ ìˆœìœ„**: {priority}\n")
                report_content.append(f"**ì˜ˆìƒ ë¹ˆë„**: {frequency}\n\n")
                
                # ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ì°¸ì¡°
                report_content.append(f"![ì›Œë“œí´ë¼ìš°ë“œ](wordclouds/topic_{topic_id}_wordcloud.png)\n\n")
                
                report_content.append("---\n\n")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = f"{output_dir}/reports/topic_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(''.join(report_content))
        
        print(f"ì£¼ì œë³„ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")
        return report_path
    
    def analyze_all_topics(self, csv_dir: str, output_dir: str) -> Dict[str, Any]:
        """ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ìƒì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        print("=" * 60)
        print("ì£¼ì œë³„ ìƒì„¸ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        # ë°ì´í„° ë¡œë“œ
        topic_assignments, topic_keywords, preprocessed_data = self.load_topic_data(csv_dir)
        
        # ì£¼ì œë³„ í†µê³„ ìƒì„±
        topic_stats = self.create_topic_statistics(topic_assignments, topic_keywords)
        topic_stats.to_csv(f"{output_dir}/csv/topic_detailed_statistics.csv", 
                          index=False, encoding='utf-8-sig')
        
        # ì£¼ì œë³„ ìƒì„¸ ë¶„ì„
        topic_analyses = {}
        
        for topic_id in topic_assignments['topic_id'].unique():
            print(f"\n{'='*20} ì£¼ì œ {topic_id} ë¶„ì„ {'='*20}")
            
            # í•´ë‹¹ ì£¼ì œì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            topic_texts = self.extract_topic_texts(topic_assignments, preprocessed_data, topic_id)
            
            if not topic_texts:
                print(f"ì£¼ì œ {topic_id}ì— í• ë‹¹ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ì£¼ì œëª…ê³¼ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
            topic_docs = topic_assignments[topic_assignments['topic_id'] == topic_id]
            topic_name = topic_docs['topic_name'].iloc[0]
            
            topic_kw = topic_keywords[topic_keywords['topic_id'] == topic_id]
            topic_keywords_list = topic_kw.nlargest(10, 'score')['term'].tolist()
            
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            self.create_topic_wordcloud(topic_texts, topic_name, output_dir, topic_id)
            
            # LLM ë¶„ì„
            analysis_result = self.analyze_topic_with_llm(topic_texts, topic_name, topic_keywords_list)
            topic_analyses[topic_id] = analysis_result
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
            time.sleep(0.5)
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ (í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
        topic_analyses_str_keys = {str(k): v for k, v in topic_analyses.items()}
        with open(f"{output_dir}/csv/topic_analyses.json", 'w', encoding='utf-8') as f:
            json.dump(topic_analyses_str_keys, f, ensure_ascii=False, indent=2)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_topic_report(topic_analyses, topic_stats, output_dir)
        
        print("\n" + "=" * 60)
        print("ì£¼ì œë³„ ìƒì„¸ ë¶„ì„ ì™„ë£Œ")
        print("=" * 60)
        
        return {
            'topic_analyses': topic_analyses,
            'topic_stats': topic_stats,
            'total_topics': len(topic_analyses)
        } 