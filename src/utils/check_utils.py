#!/usr/bin/env python3
"""
ì²´í¬ ë° í…ŒìŠ¤íŠ¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
ë°ì´í„° ê²€ì¦, ê²°ê³¼ í™•ì¸, ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import pandas as pd
import json
import os
from pathlib import Path
from typing import Dict, Any, List

def check_data_integrity(data_path: str) -> Dict[str, Any]:
    """ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
    print("ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘...")
    
    try:
        df = pd.read_excel(data_path)
        
        # ê¸°ë³¸ ì •ë³´
        info = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'missing_values': df.isnull().sum().to_dict(),
            'column_names': df.columns.tolist(),
            'data_types': df.dtypes.to_dict()
        }
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['ì—°ë²ˆ', 'ìƒë‹´ì¼ì', 'ìƒë‹´ìœ í˜•', 'ìƒë‹´ìš”ì•½', 'ìƒë‹´ì¸ ìœ í˜•', 'ìƒë‹´ë‚´ìš©']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        info['missing_required_columns'] = missing_columns
        info['data_valid'] = len(missing_columns) == 0
        
        print(f"ë°ì´í„° ê²€ì‚¬ ì™„ë£Œ: {info['total_rows']}í–‰, {info['total_columns']}ì—´")
        return info
        
    except Exception as e:
        print(f"ë°ì´í„° ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        return {'error': str(e), 'data_valid': False}

def check_topic_results(output_dir: str) -> Dict[str, Any]:
    """ì£¼ì œ ë°œê²¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("ì£¼ì œ ë°œê²¬ ê²°ê³¼ í™•ì¸ ì¤‘...")
    
    results = {}
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    files_to_check = [
        'topic_assignments.csv',
        'global_topics_top_terms.csv',
        'topic_summary.csv'
    ]
    
    for file_name in files_to_check:
        file_path = Path(output_dir) / 'csv' / file_name
        results[file_name] = file_path.exists()
        
        if file_path.exists():
            try:
                df = pd.read_csv(file_path)
                results[f"{file_name}_rows"] = len(df)
            except Exception as e:
                results[f"{file_name}_error"] = str(e)
    
    # ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ í™•ì¸
    if results.get('topic_assignments.csv'):
        try:
            assignments = pd.read_csv(Path(output_dir) / 'csv' / 'topic_assignments.csv')
            topic_counts = assignments['topic_id'].value_counts().to_dict()
            results['topic_distribution'] = topic_counts
            results['total_topics'] = len(topic_counts)
        except Exception as e:
            results['topic_distribution_error'] = str(e)
    
    print(f"ì£¼ì œ ë°œê²¬ ê²°ê³¼ í™•ì¸ ì™„ë£Œ: {results.get('total_topics', 0)}ê°œ ì£¼ì œ")
    return results

def check_clustering_results(output_dir: str) -> Dict[str, Any]:
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í™•ì¸ ì¤‘...")
    
    results = {}
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    files_to_check = [
        'cluster_assignments.csv',
        'cluster_summary.csv'
    ]
    
    for file_name in files_to_check:
        file_path = Path(output_dir) / 'csv' / file_name
        results[file_name] = file_path.exists()
        
        if file_path.exists():
            try:
                df = pd.read_csv(file_path)
                results[f"{file_name}_rows"] = len(df)
            except Exception as e:
                results[f"{file_name}_error"] = str(e)
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ìˆ˜ í™•ì¸
    if results.get('cluster_assignments.csv'):
        try:
            assignments = pd.read_csv(Path(output_dir) / 'csv' / 'cluster_assignments.csv')
            cluster_counts = assignments['cluster_name'].value_counts().to_dict()
            results['cluster_distribution'] = cluster_counts
            results['total_clusters'] = len(cluster_counts)
        except Exception as e:
            results['cluster_distribution_error'] = str(e)
    
    print(f"í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í™•ì¸ ì™„ë£Œ: {results.get('total_clusters', 0)}ê°œ í´ëŸ¬ìŠ¤í„°")
    return results

def check_llm_analysis_results(output_dir: str) -> Dict[str, Any]:
    """LLM ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("LLM ë¶„ì„ ê²°ê³¼ í™•ì¸ ì¤‘...")
    
    results = {}
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    files_to_check = [
        'analysis_results.json',
        'individual_analyses.csv',
        'cluster_analyses.csv'
    ]
    
    for file_name in files_to_check:
        file_path = Path(output_dir) / 'csv' / file_name
        results[file_name] = file_path.exists()
        
        if file_path.exists():
            try:
                if file_name.endswith('.json'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    results[f"{file_name}_items"] = len(data) if isinstance(data, list) else 1
                else:
                    df = pd.read_csv(file_path)
                    results[f"{file_name}_rows"] = len(df)
            except Exception as e:
                results[f"{file_name}_error"] = str(e)
    
    print("LLM ë¶„ì„ ê²°ê³¼ í™•ì¸ ì™„ë£Œ")
    return results

def check_topic_analysis_results(output_dir: str) -> Dict[str, Any]:
    """ì£¼ì œë³„ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print("ì£¼ì œë³„ ë¶„ì„ ê²°ê³¼ í™•ì¸ ì¤‘...")
    
    results = {}
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    files_to_check = [
        'topic_analyses.json',
        'topic_detailed_statistics.csv'
    ]
    
    for file_name in files_to_check:
        file_path = Path(output_dir) / 'csv' / file_name
        results[file_name] = file_path.exists()
        
        if file_path.exists():
            try:
                if file_name.endswith('.json'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    results[f"{file_name}_topics"] = len(data)
                else:
                    df = pd.read_csv(file_path)
                    results[f"{file_name}_rows"] = len(df)
            except Exception as e:
                results[f"{file_name}_error"] = str(e)
    
    # ì›Œë“œí´ë¼ìš°ë“œ í™•ì¸
    wordcloud_dir = Path(output_dir) / 'wordclouds'
    if wordcloud_dir.exists():
        wordcloud_files = list(wordcloud_dir.glob('*.png'))
        results['wordcloud_files'] = len(wordcloud_files)
        results['wordcloud_names'] = [f.name for f in wordcloud_files]
    
    print(f"ì£¼ì œë³„ ë¶„ì„ ê²°ê³¼ í™•ì¸ ì™„ë£Œ: {results.get('topic_analyses.json_topics', 0)}ê°œ ì£¼ì œ")
    return results

def check_all_results(output_dir: str) -> Dict[str, Any]:
    """ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."""
    print("=" * 60)
    print("ì „ì²´ ë¶„ì„ ê²°ê³¼ ì¢…í•© í™•ì¸")
    print("=" * 60)
    
    all_results = {
        'data_integrity': check_data_integrity('data_sample.xlsx'),
        'topic_discovery': check_topic_results(output_dir),
        'clustering': check_clustering_results(output_dir),
        'llm_analysis': check_llm_analysis_results(output_dir),
        'topic_analysis': check_topic_analysis_results(output_dir)
    }
    
    # ì „ì²´ ìš”ì•½
    summary = {
        'total_files_checked': sum([
            len([k for k in v.keys() if k.endswith('.csv') or k.endswith('.json')])
            for v in all_results.values()
        ]),
        'data_valid': all_results['data_integrity'].get('data_valid', False),
        'topics_found': all_results['topic_discovery'].get('total_topics', 0),
        'clusters_found': all_results['clustering'].get('total_clusters', 0),
        'llm_analysis_complete': all_results['llm_analysis'].get('analysis_results.json', False),
        'topic_analysis_complete': all_results['topic_analysis'].get('topic_analyses.json', False)
    }
    
    all_results['summary'] = summary
    
    print("\nğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½:")
    print(f"  ë°ì´í„° ìœ íš¨ì„±: {'âœ…' if summary['data_valid'] else 'âŒ'}")
    print(f"  ì£¼ì œ ë°œê²¬: {summary['topics_found']}ê°œ ì£¼ì œ")
    print(f"  í´ëŸ¬ìŠ¤í„°ë§: {summary['clusters_found']}ê°œ í´ëŸ¬ìŠ¤í„°")
    print(f"  LLM ë¶„ì„: {'âœ…' if summary['llm_analysis_complete'] else 'âŒ'}")
    print(f"  ì£¼ì œë³„ ë¶„ì„: {'âœ…' if summary['topic_analysis_complete'] else 'âŒ'}")
    
    return all_results

def print_enhanced_topic_results(output_dir: str):
    """í–¥ìƒëœ ì£¼ì œ ë°œê²¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("=" * 60)
    print("ğŸ¯ LDA ì£¼ì œ ë°œê²¬ ê²°ê³¼ (ìµœì  ì£¼ì œ ìˆ˜ ìë™ íƒìƒ‰)")
    print("=" * 60)
    
    try:
        # ì£¼ì œë³„ ìƒìœ„ í‚¤ì›Œë“œ ë¡œë“œ
        df = pd.read_csv(f'{output_dir}/csv/global_topics_top_terms.csv')
        assignments_df = pd.read_csv(f'{output_dir}/csv/topic_assignments.csv')
        
        # ì£¼ì œë³„ ì •ë³´ ì¶œë ¥
        for topic_id in df['topic_id'].unique():
            topic_df = df[df['topic_id'] == topic_id]
            topic_name = topic_df['topic_name'].iloc[0]
            
            # í•´ë‹¹ ì£¼ì œì— í• ë‹¹ëœ ë¬¸ì„œë“¤
            topic_assignments = assignments_df[assignments_df['topic_id'] == topic_id]
            
            print(f"\nğŸ“Œ {topic_name}")
            print(f"   ğŸ“Š í¬í•¨ëœ ë‚´ìš© ê°œìˆ˜: {len(topic_assignments)}ê°œ")
            print(f"   ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(topic_df['term'].head(5).tolist())}")
            
            # ì£¼ì œì— í¬í•¨ëœ ë‚´ìš©ë“¤
            print(f"   ğŸ“ í¬í•¨ëœ ë‚´ìš©:")
            for idx, row in topic_assignments.iterrows():
                print(f"      {row['document_id']+1}. {row['text']}")
            
            print("-" * 50)
        
        # ì „ì²´ í†µê³„
        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   ì´ ë¬¸ì„œ ìˆ˜: {len(assignments_df)}ê°œ")
        print(f"   ë°œê²¬ëœ ì£¼ì œ ìˆ˜: {len(df['topic_id'].unique())}ê°œ")
        
        # ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ ë¶„í¬
        topic_counts = assignments_df['topic_name'].value_counts()
        print(f"   ì£¼ì œë³„ ë¶„í¬:")
        for topic_name, count in topic_counts.items():
            print(f"      {topic_name}: {count}ê°œ")
            
    except FileNotFoundError as e:
        print(f"ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ë¨¼ì € run_02_topic_discovery.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    # ì „ì²´ ê²°ê³¼ í™•ì¸
    results = check_all_results("outputs")
    
    # í–¥ìƒëœ ì£¼ì œ ê²°ê³¼ ì¶œë ¥
    print_enhanced_topic_results("outputs") 